# Amazon Kinesis

Amazon Kinesis is not a database, but a suite of services to collect, process, and analyze real-time, streaming data. 
These services enable you to process and analyze data as it arrives, and respond instantly instead of having to wait 
until all your data is collected before the processing can begin.

There are two primary parts to Kinesis: Kinesis Data Streams and Kinesis Data Firehose.

## Kinesis Data Streams
This service lets you collect and process large streams of data records in real time. You can create data-processing 
applications, known as Kinesis Data Streams applications, and you can use them to continuously read data from your data 
streams.

## Kinesis Data Firehose
This service captures, transforms, and loads streaming data into data lakes, databases, and data stores.

## Use Cases
1) **Real-Time Analytics**: such as for social media, application logs, IoT sensor data, etc.
2) **Mobile App Data Collection**: Capture data generated by mobile apps in real-time for analytics. 
3) **Real-Time Dashboards**: Build real-time dashboards for monitoring. 
4) **Log and Event Data Collection**: Centralize logs from your servers in real-time.

## Pros
1) **Scalability**: Amazon Kinesis can handle any amount of streaming data and process data from hundreds of thousands 
of sources. 
2) **Real-time**: With Kinesis, you can analyze data and get insights in seconds or minutes. 
3) **Flexibility**: Kinesis provides multiple ways to process data (Lambda, EC2, on-premises).

## Cons
1) **Cost**: Depending on the scale of usage, costs can increase significantly.
2) **Complexity**: Setting up and optimizing Kinesis might be complex for some users.
3) **Limited Outside AWS**: Its functionality might be limited if you use infrastructure outside of AWS.

## Comparisons with competition

### Hosting and Operational Effort
- **Apache Kafka**:  It's an open-source project which can be installed and run on any infrastructure, including 
on-premises servers or any cloud platform. However, it requires efforts to set up, maintain and scale.
- **Google Cloud Pub/Sub**: Google manages all operational aspects of Pub/Sub so that you can focus on application
development. It is designed to provide reliable, many-to-many, asynchronous messaging between applications.
- **Azure Event Hubs**: Like Pub/Sub, Azure manages Event Hubs. You can scale automatically and only pay for what you
use.
- **Amazon Kinesis**: Like Google Cloud Pub/Sub and Azure Event Hubs, Kinesis is fully managed by AWS. This eliminates 
the operational effort required to set up and maintain the infrastructure.

### Data Throughput and System Performance
- **Apache Kafka**: Kafka can handle very high volume real-time message feed and has better throughput in comparison to
others.
- **Google Cloud Pub/Sub**: Pub/Sub may not match the performance of Kafka, but it does scale automatically according to 
the inbound data and supports auto load balancing.
- **Azure Event Hubs**: Event Hubs can handle millions of events per second, allowing rapid data intake that can be 
processed further down the pipeline using other Azure services like Stream Analytics.
- **Amazon Kinesis**: Amazon Kinesis can handle any amount of streaming data and process it from hundreds of thousands
of sources with very low latencies.

### Latency
- **Apache Kafka**: Kafka guarantees low latency in the milliseconds, making it ideal for real-time analytic workloads.
- **Google Cloud Pub/Sub**: It also ensures low-latency delivery of messages, but the focus is more on global-scale 
messaging.
- **Azure Event Hubs**: A small latency is involved, but it is generally suited for real-time and near-real-time 
analytic workloads.
- **Amazon Kinesis**: Offers low latencies, making it ideal for real-time data and analytics use cases.

### Fault tolerance
- **Apache Kafka**: Kafka provides fault tolerance by maintaining the state of consumers' progress in topics through
"offsets," which are sequential IDs Kafka assigns to records as they arrive. Consumers commit these offsets to a Kafka 
topic - "_consumer_offsets" - which allows them to keep track of the records they have read in case of a failover. The 
consumer's position (called "offset watermark") can be committed in a synchronous or asynchronous manner, and in case of 
a failure, the consumer can resume processing from the last committed offset, guaranteeing at-least-once processing
semantics.
- **Google Cloud Pub/Sub**: Google Cloud Pub/Sub provides a pull-based model in which the client library keeps track of 
the acknowledgment deadline for each message and extends them as needed, referred to as "lease management." Once a 
message is successfully processed, the client sends an acknowledgment to Pub/Sub, ensuring the message is not 
redelivered, and the unacknowledged message is used as a watermark to resume processing in case of a failure. Fault 
tolerance is maintained as messages will continue to be redelivered until an acknowledgment is received, ensuring 
at-least-once processing semantics.
- **Azure Event Hubs**: Azure Event Hubs stores events in the sequence they're received, which forms the basis for its 
sequence number, called "offset," that's unique to each partition and used as a watermark to track processing state. 
Event Hubs uses checkpointing to persist the offset of the last event processed plus one, recording it to an associated 
Azure Blob Storage, providing fault tolerance by allowing processors to resume where they left off on restart. The 
balance between checkpoint frequency and performance versus fault tolerance needs to be tuned; the less frequent the 
checkpoints, the more potential reprocessing needed after an application crash.
- **Amazon Kinesis**: Amazon Kinesis leverages sequence numbers for records that are unique, incremental identifiers 
assigned to every record, acting as a watermark to track the state of stream reading. Checkpointing is the mechanism 
used by Kinesis to handle failures; it logs the sequence number of the last record processed to a DynamoDB table, 
effectively providing a save point in the stream that a consumer can return to on failure. On recovery from a failure, 
the consumer can start processing from the last successful checkpoint, reprocess records, and ensure no data loss, 
providing at-least-once processing semantics.

#### Watermarking
Tracking the watermark, commonly done through mechanisms like committing offsets in Kafka or checkpointing in Event Hubs
and Kinesis, is indeed controlled by the consumer or the application processing the events.

How frequently this tracking is done is typically driven by the trade-off between fault tolerance and performance:
- More frequent tracking (committing or checkpointing after every message) ensures minimal loss of progress in case of 
a failure, but can impose additional overhead due to the increased number of system calls, directly impacting 
performance or throughput.
- Less frequent tracking (committing or checkpointing after processing a batch of messages) enhances performance but 
increases the possibility of having to re-process more messages if a fault were to occur.

Therefore, it's up to the consumer to determine the optimal frequency based on its tolerance for re-processing and 
performance needs. This often depends on the specific use-case and nature of processing performed by the consumer. This 
is a common design decision that needs to be made when working with any stream processing system.

### Use Cases
- **Apache Kafka**: It is used when there's a need for high-speed filtering, monitoring, or tracking of streaming data 
in real-time. It is used by LinkedIn, Uber, Twitter, and more for activities like tracking website activity and 
infrastructure monitoring.
- **Google Cloud Pub/Sub**: It provides durable, low-latency messaging for cloud-native applications, and is used as 
part of larger data and analytics pipelines for distributed event ingestion and delivery systems, user analytics, and 
IoT device telemetry.
- **Azure Event Hubs**: Good for telemetry, distributed data streaming, real-time analytics, and archiving data. Popular 
use-cases include anomaly detection, payment processing, fraud detection, and telemetry from distributed services.
- **Amazon Kinesis**: Kinesis is commonly used for real-time analytics, log and event data collection, real-time 
dashboard creation, and IoT device telemetry.

### Pricing Models
- **Apache Kafka**: As an open-source tool, it's free to use. However, you may incur costs from the infrastructure where 
it's installed and operationally maintained.
- **Google Cloud Pub/Sub**: You pay for the amount of data sent and received. There are no upfront costs, termination 
fees, or licensing fees.
- **Azure Event Hubs**: Microsoft charges based on the throughput units, data retention period, and captured data size. 
There is also a cost associated with executing operations.
- **Amazon Kinesis**: For Kinesis, you pay for the volume of data ingested, and also for optional features like data 
retention and enhanced fan-out capabilities.

### Hosting and Management
Google Cloud Pub/Sub, Azure Event Hubs, and Amazon Kinesis are fully managed services, while Apache Kafka requires 
setting up and managing your own infrastructure.

### Scalability and Performance
All the four services can handle high volumes of data, but Kafka stands out for its superior performance under huge 
loads.